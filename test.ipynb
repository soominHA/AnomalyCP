{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import AnomalyCLIP_lib\n",
    "import torch\n",
    "import argparse\n",
    "import torch.nn.functional as F\n",
    "from prompt_ensemble import AnomalyCLIP_PromptLearner\n",
    "from loss import FocalLoss, BinaryDiceLoss\n",
    "from utils import normalize\n",
    "from dataset import Dataset\n",
    "from logger import get_logger\n",
    "from tqdm import tqdm\n",
    "\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "from tabulate import tabulate\n",
    "from utils import get_transform\n",
    "\n",
    "def setup_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "from visualization import visualizer\n",
    "\n",
    "from metrics import image_level_metrics, pixel_level_metrics\n",
    "from tqdm import tqdm\n",
    "from scipy.ndimage import gaussian_filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"C:/Users/dmkwo/Documents/soo/AnomalyCLIP/data/orange\"\n",
    "save_path = \"./results/9_12_4_multiscale_orange\"\n",
    "checkpoint_path = \"./checkpoints/9_12_4_multiscale/epoch_15.pth\"\n",
    "dataset = \"orange\"\n",
    "features_list = [6, 12, 18, 24]\n",
    "image_size = 518\n",
    "depth = 9\n",
    "n_ctx = 12\n",
    "t_n_ctx = 4\n",
    "feature_map_layer = [0, 1, 2, 3]\n",
    "metrics = 'image-pixel-level'\n",
    "seed = 111\n",
    "sigma = 4\n",
    "\n",
    "# args = {\"data_path\":data_path, \"save_path\":save_path, \"checkpoint_path\":checkpoint_path, \"dataset\":dataset, \n",
    "#         \"features_list\":features_list, \"image_size\":image_size, \"depth\":depth, \"n_ctx\":n_ctx, \"t_n_ctx\":t_n_ctx, \n",
    "#         \"feature_map_layer\":feature_map_layer, \"metrics\":metrics, \"seed\":seed, \"sigma\":sigma}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(\"AnomalyCLIP\", add_help=True)\n",
    "# paths\n",
    "parser.add_argument(\"--data_path\", type=str, default=\"./data/orange\", help=\"path to test dataset\")\n",
    "parser.add_argument(\"--save_path\", type=str, default='./results/9_12_4_multiscale_orange', help='path to save results')\n",
    "parser.add_argument(\"--checkpoint_path\", type=str, default='./checkpoints/9_12_4_multiscale/epoch_15.pth', help='path to checkpoint')\n",
    "# model\n",
    "parser.add_argument(\"--dataset\", type=str, default='orange')\n",
    "parser.add_argument(\"--features_list\", type=int, nargs=\"+\", default=[6, 12, 18, 24], help=\"features used\")\n",
    "parser.add_argument(\"--image_size\", type=int, default=518, help=\"image size\")\n",
    "parser.add_argument(\"--depth\", type=int, default=9, help=\"image size\")\n",
    "parser.add_argument(\"--n_ctx\", type=int, default=12, help=\"zero shot\")\n",
    "parser.add_argument(\"--t_n_ctx\", type=int, default=4, help=\"zero shot\")\n",
    "parser.add_argument(\"--feature_map_layer\", type=int,  nargs=\"+\", default=[0, 1, 2, 3], help=\"zero shot\")\n",
    "parser.add_argument(\"--metrics\", type=str, default='image-pixel-level')\n",
    "parser.add_argument(\"--seed\", type=int, default=111, help=\"random seed\")\n",
    "parser.add_argument(\"--sigma\", type=int, default=4, help=\"zero shot\")\n",
    "\n",
    "args = parser.parse_args(args=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = get_logger(args.save_path)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "AnomalyCLIP_parameters = {\"Prompt_length\": args.n_ctx, \"learnabel_text_embedding_depth\": args.depth, \"learnabel_text_embedding_length\": args.t_n_ctx}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name ViT-L/14@336px\n",
      "text_layer False\n",
      "text_layer True\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AnomalyCLIP(\n",
       "  (visual): VisionTransformer(\n",
       "    (conv1): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)\n",
       "    (ln_pre): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    (transformer): Transformer(\n",
       "      (resblocks): ModuleList(\n",
       "        (0): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (1): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (2): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (3): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (4): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (5): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (6): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (7): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (8): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (9): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (10): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (11): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (12): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (13): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (14): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (15): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (16): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (17): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (18): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (19): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (20): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (21): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (22): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (23): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_post): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (transformer): Transformer(\n",
       "    (resblocks): ModuleList(\n",
       "      (0): ResidualAttentionBlock_learnable_token(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (1): ResidualAttentionBlock_learnable_token(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (2): ResidualAttentionBlock_learnable_token(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (3): ResidualAttentionBlock_learnable_token(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (4): ResidualAttentionBlock_learnable_token(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (5): ResidualAttentionBlock_learnable_token(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (6): ResidualAttentionBlock_learnable_token(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (7): ResidualAttentionBlock_learnable_token(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (8): ResidualAttentionBlock_learnable_token(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (9): ResidualAttentionBlock_learnable_token(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (10): ResidualAttentionBlock_learnable_token(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (11): ResidualAttentionBlock_learnable_token(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (token_embedding): Embedding(49408, 768)\n",
       "  (ln_final): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       ")"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model, _ = AnomalyCLIP_lib.load(\"ViT-L/14@336px\", device=device, design_details = AnomalyCLIP_parameters)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess, target_transform = get_transform(args)\n",
    "test_data = Dataset(root=args.data_path, transform=preprocess, target_transform=target_transform, dataset_name = args.dataset)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_data, batch_size=1, shuffle=False)\n",
    "obj_list = test_data.obj_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['orange']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obj_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "metrics = {}\n",
    "for obj in obj_list:\n",
    "    results[obj] = {}\n",
    "    results[obj]['gt_sp'] = []\n",
    "    results[obj]['pr_sp'] = []\n",
    "    results[obj]['imgs_masks'] = []\n",
    "    results[obj]['anomaly_maps'] = []\n",
    "    metrics[obj] = {}\n",
    "    metrics[obj]['pixel-auroc'] = 0\n",
    "    metrics[obj]['pixel-aupro'] = 0\n",
    "    metrics[obj]['image-auroc'] = 0\n",
    "    metrics[obj]['image-ap'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing class-specific contexts\n",
      "single_para torch.Size([4, 768])\n",
      "single_para torch.Size([4, 768])\n",
      "single_para torch.Size([4, 768])\n",
      "single_para torch.Size([4, 768])\n",
      "single_para torch.Size([4, 768])\n",
      "single_para torch.Size([4, 768])\n",
      "single_para torch.Size([4, 768])\n",
      "single_para torch.Size([4, 768])\n",
      "embedding_pos torch.Size([1, 77, 768])\n",
      "tokenized_prompts shape torch.Size([1, 1, 77]) torch.Size([1, 1, 77])\n"
     ]
    }
   ],
   "source": [
    "prompt_learner = AnomalyCLIP_PromptLearner(model.to(\"cpu\"), AnomalyCLIP_parameters)\n",
    "checkpoint = torch.load(args.checkpoint_path)\n",
    "prompt_learner.load_state_dict(checkpoint[\"prompt_learner\"])\n",
    "prompt_learner.to(device)\n",
    "model.to(device)\n",
    "model.visual.DAPM_replace(DPAM_layer = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***\n",
      "tensor([[[ 0.0012,  0.0032,  0.0003,  ..., -0.0019,  0.0003,  0.0019],\n",
      "         [-0.0187, -0.0407, -0.0552,  ..., -0.0271, -0.0147,  0.0269],\n",
      "         [-0.0360,  0.0567, -0.0307,  ..., -0.0068,  0.0393,  0.0139],\n",
      "         ...,\n",
      "         [-0.0015,  0.0360,  0.0223,  ...,  0.0148,  0.0045, -0.0214],\n",
      "         [-0.0015,  0.0360,  0.0223,  ...,  0.0148,  0.0045, -0.0214],\n",
      "         [-0.0015,  0.0360,  0.0223,  ...,  0.0148,  0.0045, -0.0214]],\n",
      "\n",
      "        [[ 0.0012,  0.0032,  0.0003,  ..., -0.0019,  0.0003,  0.0019],\n",
      "         [ 0.0244,  0.1180,  0.0415,  ..., -0.0288,  0.0463,  0.0488],\n",
      "         [ 0.0578,  0.0428,  0.0656,  ...,  0.0013,  0.0128,  0.0003],\n",
      "         ...,\n",
      "         [-0.0015,  0.0360,  0.0223,  ...,  0.0148,  0.0045, -0.0214],\n",
      "         [-0.0015,  0.0360,  0.0223,  ...,  0.0148,  0.0045, -0.0214],\n",
      "         [-0.0015,  0.0360,  0.0223,  ...,  0.0148,  0.0045, -0.0214]]],\n",
      "       device='cuda:0', grad_fn=<CatBackward0>)\n",
      "tensor([[49406,   343,   343,   343,   343,   343,   343,   343,   343,   343,\n",
      "           343,   343,   343, 14115,   269, 49407,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0],\n",
      "        [49406,   343,   343,   343,   343,   343,   343,   343,   343,   343,\n",
      "           343,   343,   343, 13568, 14115,   269, 49407,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0]], device='cuda:0',\n",
      "       dtype=torch.int32)\n",
      "ParameterList(\n",
      "    (0): Parameter containing: [torch.cuda.FloatTensor of size 4x768 (GPU 0)]\n",
      "    (1): Parameter containing: [torch.cuda.FloatTensor of size 4x768 (GPU 0)]\n",
      "    (2): Parameter containing: [torch.cuda.FloatTensor of size 4x768 (GPU 0)]\n",
      "    (3): Parameter containing: [torch.cuda.FloatTensor of size 4x768 (GPU 0)]\n",
      "    (4): Parameter containing: [torch.cuda.FloatTensor of size 4x768 (GPU 0)]\n",
      "    (5): Parameter containing: [torch.cuda.FloatTensor of size 4x768 (GPU 0)]\n",
      "    (6): Parameter containing: [torch.cuda.FloatTensor of size 4x768 (GPU 0)]\n",
      "    (7): Parameter containing: [torch.cuda.FloatTensor of size 4x768 (GPU 0)]\n",
      ")\n",
      " *** \n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompts, tokenized_prompts, compound_prompts_text = prompt_learner(cls_id = None)\n",
    "\n",
    "print(f'***\\n{prompts}\\n{tokenized_prompts}\\n{compound_prompts_text}\\n *** \\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.0411, -0.0154,  0.0404,  ..., -0.0621, -0.0004, -0.0997],\n",
      "         [-0.0425,  0.0133, -0.0352,  ...,  0.0522,  0.0033,  0.0958]]],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "text_features = model.encode_text_learn(prompts, tokenized_prompts, compound_prompts_text).float()\n",
    "text_features = torch.stack(torch.chunk(text_features, dim = 0, chunks = 2), dim = 1)\n",
    "text_features = text_features/text_features.norm(dim=-1, keepdim=True)\n",
    "print(text_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.0411, -0.0425],\n",
      "         [-0.0154,  0.0133],\n",
      "         [ 0.0404, -0.0352],\n",
      "         ...,\n",
      "         [-0.0621,  0.0522],\n",
      "         [-0.0004,  0.0033],\n",
      "         [-0.0997,  0.0958]]], device='cuda:0', grad_fn=<PermuteBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(text_features.permute(0, 2, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/99 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " *** \n",
      " image_features: tensor([[ 5.9967e-03,  7.5851e-01,  2.6046e-01,  4.7619e-02, -3.7095e-01,\n",
      "         -4.8167e-02,  3.1094e-02, -2.2805e-02,  1.8683e-01,  4.3864e-02,\n",
      "          1.9436e-01, -3.7653e-01, -6.5187e-02,  5.2986e-01,  1.0068e-01,\n",
      "         -1.2386e-01, -4.1698e-01, -2.3728e-01,  3.4110e-01, -2.7846e-01,\n",
      "          3.9896e-02,  1.0191e-01,  1.2274e-01,  2.1334e-01, -5.7332e-01,\n",
      "         -1.1138e-01, -2.6434e-01, -4.1776e-01,  1.9270e-01, -3.4563e-02,\n",
      "          2.2431e-01, -1.7430e-01, -3.1606e-01, -3.8469e-02,  1.7556e-01,\n",
      "          3.6981e-01, -6.6115e-02, -9.8012e-02,  2.1256e-01, -1.6106e-02,\n",
      "          8.4564e-02, -7.2324e-02,  3.2877e-01, -4.4942e-01, -2.7108e-01,\n",
      "          1.6757e-01, -2.6343e-02,  3.2838e-01,  8.7209e-02, -3.6387e-02,\n",
      "          1.6057e-01,  6.5648e-02,  1.7208e-01,  8.2174e-02, -3.5652e-01,\n",
      "         -7.0663e-02,  1.4999e-01, -5.2479e-01,  4.3431e-01, -1.4094e-01,\n",
      "         -5.7409e-02, -7.8857e-02, -5.9329e-02,  5.4463e-02,  4.8163e-01,\n",
      "          2.9751e-01,  1.4405e-01,  2.1851e-01, -5.6200e-02, -3.9246e-01,\n",
      "          2.4658e-01,  7.1593e-02,  2.9299e-01, -1.0386e-01,  8.2034e-02,\n",
      "          2.3595e-01, -4.5711e-01, -1.0474e-01, -5.3097e-03,  2.3470e-01,\n",
      "          6.6148e-02,  1.2655e-01,  7.8303e-02,  1.5653e-01,  2.6579e-01,\n",
      "         -1.0187e-01,  4.6319e-01,  7.6449e-02,  6.0677e-01, -2.2390e-01,\n",
      "          2.9972e-01, -7.5310e-02,  3.0130e-01, -1.3519e-01, -7.4216e-02,\n",
      "         -2.4866e-01,  1.3572e-01,  2.6210e-03,  8.9386e-02, -4.3144e-01,\n",
      "         -2.2484e-01,  2.1072e-01, -1.4802e-01, -1.8399e-02, -1.9055e-01,\n",
      "         -2.2058e-02, -3.2492e-01, -8.5547e-02, -5.1366e-01, -3.8690e-01,\n",
      "         -6.8889e-02,  9.8196e-03,  1.5415e-01, -3.3009e-01, -7.3996e-02,\n",
      "         -4.8703e-02, -1.6314e-01, -2.6923e-01, -3.1535e-01,  1.1322e-01,\n",
      "         -1.9649e-01, -5.0975e-02,  2.9131e-01,  6.2463e-02, -6.9259e-02,\n",
      "         -1.5607e-02,  1.0929e-01, -1.5256e-01, -2.7404e-01,  1.2420e-01,\n",
      "         -2.5976e-01,  3.0937e-01,  7.1737e-02, -9.0576e-02, -5.1454e-02,\n",
      "          2.8070e-01, -3.4270e-01, -9.0383e-02,  4.3212e-01,  1.8349e-02,\n",
      "         -2.4601e-02,  1.0768e-01, -3.3516e-01,  1.3655e-02,  3.9676e-01,\n",
      "          2.6121e-01, -3.8416e-01, -1.8517e-01, -2.8113e-01,  2.3383e-01,\n",
      "         -4.0182e-01,  3.5263e-01,  3.1449e-01,  3.7485e-02, -2.8923e-01,\n",
      "          2.7906e-01,  1.8196e-01,  1.5304e-01,  9.6019e-02, -2.4155e-01,\n",
      "          4.9334e-02,  2.6218e-02,  1.3228e-01, -2.7756e-01, -2.1851e-02,\n",
      "         -7.4724e-02, -1.5784e-01,  1.0695e-01, -3.4941e-01,  1.7095e-01,\n",
      "          2.0077e-01, -8.5392e-02,  2.5137e-01,  4.0092e-01, -3.6670e-01,\n",
      "         -1.2298e-01, -1.5364e-01,  7.3168e-01,  6.6174e-01,  2.1790e-01,\n",
      "          6.5876e-01,  4.5701e-03, -6.6901e-02, -2.3141e-01,  5.6419e-02,\n",
      "         -1.1663e-02, -6.1484e-01,  2.1630e-01, -7.1763e-02, -1.4058e-01,\n",
      "         -2.6389e-02,  2.1038e-01,  6.0342e-02, -1.4022e-01, -2.9571e-01,\n",
      "          3.5739e-01, -3.3247e-01,  4.7441e-01,  3.4031e-01, -7.9108e-02,\n",
      "         -2.8605e-02, -1.1063e+00,  1.1709e-01,  1.1475e-02, -3.6456e-01,\n",
      "          4.7187e-01,  1.1697e-01, -6.3530e-02,  1.9023e-01,  1.7471e-01,\n",
      "          2.1242e-01, -1.9167e-01, -7.4947e-02,  2.6292e-01,  1.5804e-01,\n",
      "          2.8824e-01,  4.5554e-02, -3.2102e-01, -1.9639e-02, -3.3959e-01,\n",
      "         -3.4623e-01, -3.2607e-01,  1.5445e-01,  1.9947e-01, -2.1098e-01,\n",
      "         -7.8308e-02, -1.1085e-01, -2.2330e-01,  3.8094e-01, -8.5021e-02,\n",
      "         -5.5202e-01, -6.5927e-02,  2.1568e-01, -6.2704e-01,  2.5707e-01,\n",
      "          2.3056e-01, -6.8825e-02, -3.2289e-01,  3.5757e-01,  1.7565e-01,\n",
      "         -4.5546e-01,  2.8620e-01,  9.7772e-02, -2.3271e-01, -5.1032e-01,\n",
      "          4.8583e-02, -1.9010e-01,  1.3431e-01,  1.7035e-03,  2.5482e-01,\n",
      "          2.9894e-01, -1.6589e-02,  3.4795e-01,  6.2648e-02, -2.0921e-01,\n",
      "         -6.8455e-02,  2.3001e-01,  5.0846e-01, -3.3352e-01,  6.0596e-01,\n",
      "         -2.8931e-01, -3.8296e-01,  1.2344e-01, -2.5413e-02, -2.7510e-01,\n",
      "          1.5943e-01,  1.5279e-01,  6.6689e-01, -5.4209e-01, -1.2501e-01,\n",
      "          3.0852e-02,  4.4440e-01, -5.5278e-02,  2.7914e-01,  8.3915e-02,\n",
      "         -5.4913e-02,  3.3905e-03,  6.4009e-02,  2.2708e-01,  1.8982e-01,\n",
      "         -1.7712e-01,  1.9271e-01,  2.6061e-01, -1.9680e-02, -1.1822e-01,\n",
      "          2.0076e-01,  1.6889e-01, -7.3303e-02,  3.5265e-01, -8.0692e-02,\n",
      "         -4.6959e-02, -1.3363e-01, -1.3463e-01,  3.3293e-01,  4.8661e-01,\n",
      "         -7.5053e-02, -1.0776e-01, -7.6608e-02, -4.2894e-01, -1.0802e-02,\n",
      "          4.1297e-01,  4.5787e-02, -1.3479e-02,  2.0601e-01,  7.2300e-02,\n",
      "          4.2383e-01, -2.2745e-01,  2.4579e-01, -4.1188e-01,  3.1839e-01,\n",
      "          1.9815e-01,  5.9059e-02,  1.7060e-01,  3.6730e-01,  2.0476e-01,\n",
      "          4.4413e-02, -2.1945e-01,  1.6930e-01, -2.7069e-02,  2.6751e-01,\n",
      "         -3.0876e-01, -2.1743e-01, -1.8977e-01,  4.4018e-01, -5.0487e-03,\n",
      "          3.5123e-02, -1.9049e-01, -6.0285e-01, -3.5129e-01,  5.0773e-02,\n",
      "          4.6684e-01, -1.2267e-01,  1.2068e-02, -3.3202e-01, -3.7030e-01,\n",
      "         -3.9450e-02, -3.7937e-01,  1.1887e-02, -3.1192e-01,  1.3542e-01,\n",
      "         -1.7408e-01, -1.5947e-01,  2.2241e-01, -2.7120e-01, -1.2359e-01,\n",
      "         -1.3275e-01,  1.1059e-01, -6.3516e-02,  4.0247e-02, -3.3779e-01,\n",
      "          2.2023e-01,  5.8931e-02, -6.0301e-01,  1.0898e-01, -1.4852e-01,\n",
      "         -2.2121e-01,  7.5573e-02, -1.1099e-01,  2.8683e-01, -1.1221e-01,\n",
      "          3.2342e-01,  2.6459e-01,  1.5964e-01,  6.1447e-01, -2.6383e-01,\n",
      "         -1.2670e-01,  1.6090e-01, -3.1484e-01,  3.7985e-01, -2.1387e-01,\n",
      "         -2.9082e-01,  1.0699e-01, -5.5792e-01, -3.2563e-02,  3.7395e-01,\n",
      "          3.8358e-01,  7.9959e-02,  1.9924e-01,  3.8539e-01, -3.0471e-01,\n",
      "         -1.2907e-01, -1.6178e-01, -8.4006e-02, -1.9658e-01, -1.6918e-01,\n",
      "         -1.1899e-01,  2.4463e-01,  1.2833e-02,  8.8018e-03,  7.9424e-02,\n",
      "         -3.0271e-01, -1.5125e-02,  1.3939e-01, -1.9412e-01,  1.3567e-01,\n",
      "          6.6199e-02,  1.4827e-01,  1.5338e-01, -1.9736e-02,  2.5305e-01,\n",
      "          7.5621e-01,  1.3014e-01, -1.1658e-01, -2.1815e-01, -1.1507e+00,\n",
      "          7.1817e-02, -4.6266e-01,  9.7786e-02,  4.3365e-01, -1.3169e-01,\n",
      "          1.4046e-02, -6.8652e-02,  3.3927e-01, -1.9892e+00, -2.5417e-01,\n",
      "         -2.1973e-01,  2.9549e-01, -3.6248e-01,  7.0247e-01,  7.5512e-02,\n",
      "          9.4161e-03, -4.9814e-02,  9.1399e-02, -1.8653e-01,  9.4584e-02,\n",
      "         -1.3446e-01,  1.2047e-01,  1.1281e-01,  5.0834e-01,  2.3009e-01,\n",
      "          8.0020e-02,  3.3200e-01, -1.8716e-01, -1.8667e-02, -5.4773e-01,\n",
      "         -6.4289e-02,  1.4773e-01,  1.7716e-01,  4.7019e-02, -1.2999e-01,\n",
      "          9.4821e-02, -1.3917e-02,  1.1961e-01, -8.9312e-02,  2.1443e-02,\n",
      "          3.1960e-01,  1.4181e-02, -7.3996e-02,  3.0582e-02, -1.6301e-01,\n",
      "         -3.2706e-01, -1.0526e-01, -5.2407e-01,  5.2358e-01,  3.3074e-01,\n",
      "         -2.4146e-01,  1.0573e-02,  3.9031e-01,  6.7327e-02,  3.0933e-01,\n",
      "         -1.3129e-01,  1.7474e-01, -1.2558e-01, -1.2858e-03, -3.6518e-01,\n",
      "          1.5598e-01, -1.5244e-02, -6.5028e-02, -2.2457e-01, -4.7312e-01,\n",
      "          2.0653e-01,  2.1226e-01,  2.7574e-01,  7.3908e-02,  1.5036e-01,\n",
      "         -4.5298e-01,  2.3717e-01,  1.7564e-01, -6.9695e-02, -1.6545e-01,\n",
      "         -1.4911e-01,  2.8668e-01,  9.7634e-02, -1.9375e-01, -1.7372e-01,\n",
      "          6.2998e-02, -6.6930e-02, -1.3974e-01,  9.1139e-02,  1.1456e-01,\n",
      "          2.4929e-01, -2.0497e-01, -1.0374e-01,  6.7706e-01, -1.8623e-01,\n",
      "          9.7565e-03, -9.9331e-02, -2.9256e-02,  2.5469e-01,  4.2378e-01,\n",
      "          1.5533e-01,  2.3898e-02, -1.7331e-01,  2.6082e-01, -5.7017e-01,\n",
      "          8.4588e-02, -3.3776e-02, -3.4982e-02, -1.1824e-01,  1.6274e-01,\n",
      "         -2.6109e-01, -1.4101e-02,  3.7865e-01, -1.3862e-01, -1.1593e-01,\n",
      "         -1.4901e-01, -2.2737e-02,  1.1265e-01, -4.2118e-01,  2.2526e-01,\n",
      "         -5.1020e-02, -1.7521e-01,  4.0418e-01, -5.1617e-01,  2.0018e-01,\n",
      "          1.5154e-02,  4.2247e-01,  1.5154e-01, -7.1480e-01,  6.8828e-02,\n",
      "          1.5601e-01, -3.6623e-01, -1.7205e-01, -2.2591e-01,  2.8925e-01,\n",
      "         -1.4671e-01,  3.5057e-01, -9.1018e-02, -2.4397e-01,  4.9483e-01,\n",
      "          6.7993e-02,  4.1124e-01,  4.0799e-01, -4.6731e-02, -7.1047e-02,\n",
      "         -1.0225e-01,  3.6249e-01,  2.0363e-01, -2.6620e-02,  2.1900e-01,\n",
      "          8.9662e-02,  1.4601e-01,  3.6931e-01, -1.5055e-01,  8.2541e-02,\n",
      "         -3.9705e-02,  5.8159e-01,  7.9028e-02, -8.2981e-02, -7.1991e-02,\n",
      "          1.7197e-01, -5.6099e-03,  9.0264e-02,  3.2557e-01,  3.2167e-01,\n",
      "          2.5867e-01, -1.7205e-01,  1.7996e-01,  2.8223e-01,  6.3484e-03,\n",
      "          2.3072e-01, -2.2392e-01, -1.0695e-01, -8.5886e-02,  3.7039e-01,\n",
      "         -2.8422e-01, -1.4882e-01, -2.4330e-01,  1.2564e-02, -5.5915e-01,\n",
      "          2.6021e-01, -1.7460e-01, -2.7999e-01,  1.2360e-01, -3.1631e-01,\n",
      "          2.9411e-01, -1.6488e-01, -1.2624e-02,  1.3223e-02, -9.5230e-02,\n",
      "         -2.5363e-01,  4.0577e-02, -3.0885e-01, -5.0136e-01, -1.0499e-01,\n",
      "         -1.2827e-01,  4.2732e-01, -1.9228e-01, -4.3299e-01,  1.4729e-01,\n",
      "         -4.7913e-01,  1.1263e-02,  1.2569e-01, -1.1970e-01,  3.0974e-01,\n",
      "         -6.7927e-01, -1.2746e-01, -1.2275e-01, -1.5852e-01, -2.2922e-01,\n",
      "          1.7669e-01, -4.5701e-01, -7.5869e-01,  5.8872e-01,  1.3690e-01,\n",
      "          8.3518e-02,  4.6589e-01, -1.1524e-01,  3.4592e-01,  2.0538e-02,\n",
      "         -3.1994e-01, -3.1079e-02, -1.2168e-01, -2.8483e-01, -2.6704e-01,\n",
      "         -1.9108e-01,  2.5646e-01, -5.2571e-01,  4.3297e-01,  1.8037e-01,\n",
      "         -2.4633e-02, -8.6629e-02, -2.4083e-01,  2.9012e-01, -4.0095e-01,\n",
      "          5.4145e-01,  3.0097e-01, -8.7909e-01,  2.7417e-01, -4.5363e-01,\n",
      "          1.2155e-01,  4.0187e-01,  2.5309e-01, -6.7406e-02,  2.1385e+00,\n",
      "          1.9498e-01,  1.5263e-01,  4.5843e-01,  2.2139e-02, -1.9688e-02,\n",
      "         -4.2190e-01, -1.9781e-01, -2.4689e-01, -1.1492e-02, -2.1214e-01,\n",
      "          4.7906e-02, -4.7130e-02, -6.0020e-02,  3.6095e-01,  3.1872e-01,\n",
      "         -1.7941e-01, -4.1449e-01,  5.6702e-01,  2.4821e-01,  2.7038e-01,\n",
      "          1.1382e-01,  1.3212e-01, -2.8582e-01,  3.2049e-01,  1.8577e-01,\n",
      "          5.0391e-02, -7.9768e-02, -5.0012e-01,  4.8165e-02, -8.6450e-02,\n",
      "          3.0417e-01,  2.9191e-01,  3.1461e-01, -2.7443e-01, -5.4994e-01,\n",
      "         -1.1306e-01,  2.2149e-01, -2.2440e-01, -6.3263e-02, -3.0017e-01,\n",
      "         -1.1399e-02,  3.3889e-01, -1.8059e-01, -3.0817e-01, -4.8499e-02,\n",
      "         -3.5994e-02, -3.3273e-02,  1.5319e+00,  4.8061e-02, -1.0920e-01,\n",
      "          1.8498e-01, -2.8274e-01, -4.8671e-01, -5.4312e-03,  1.0214e-01,\n",
      "          5.7029e-02, -1.6462e-01, -6.2246e-02, -2.4353e-01,  4.0573e-01,\n",
      "         -2.8550e-01,  7.7577e-02, -1.4133e-01, -1.0600e-01, -1.3451e-01,\n",
      "          2.3707e-02,  2.6267e-01,  1.8103e-01, -1.2702e-02, -4.2612e-01,\n",
      "         -3.7764e-01,  3.3153e-01, -8.9697e-02, -1.1225e-01,  7.9773e-02,\n",
      "          1.1902e-01, -1.5889e-01, -2.1301e-01, -2.9290e-01, -4.3003e-02,\n",
      "         -2.3305e-01,  4.7057e-02,  3.6826e-02,  1.3192e-01,  9.8033e-02,\n",
      "         -3.3763e-01, -3.3656e-02,  3.3423e-02, -8.9987e-02,  3.7772e-01,\n",
      "          9.8341e-03,  2.5577e-01,  2.6153e-03,  7.4523e-03,  1.1967e-01,\n",
      "          1.1434e-01, -2.6890e-01,  2.3363e-01,  2.0804e-01, -4.9039e-02,\n",
      "         -4.1640e-02,  4.5153e-01, -1.8190e-01, -2.1060e-01, -9.3685e-02,\n",
      "          8.0797e-02,  3.5312e-02,  2.6225e-01, -2.6441e-01, -3.1218e-01,\n",
      "         -1.7000e-01, -8.7324e-02, -4.4440e-01, -3.1813e-01,  1.3210e-02,\n",
      "          8.1723e-02, -1.2808e-01,  3.9865e-01,  8.0530e-01, -7.1873e-02,\n",
      "          1.2175e-01, -9.3766e-02,  5.9963e-02]], device='cuda:0') \n",
      " patch_features:[tensor([[[-0.7994, -0.7280, -0.4998,  ...,  0.0102,  0.3393, -0.4293],\n",
      "         [-0.7932, -0.0890, -0.2130,  ..., -0.7117,  0.2915, -0.2182],\n",
      "         [-0.8613, -0.2703, -0.3654,  ..., -0.7263,  0.1430, -0.3292],\n",
      "         ...,\n",
      "         [-0.6305, -0.4551, -0.5146,  ..., -0.7780,  0.2512, -0.6030],\n",
      "         [-0.5449, -0.2584, -0.3540,  ..., -0.6833,  0.3796, -0.3651],\n",
      "         [-0.5614, -0.1897, -0.3873,  ..., -0.6882,  0.2852, -0.2929]]],\n",
      "       device='cuda:0'), tensor([[[-0.7994, -0.7280, -0.4998,  ...,  0.0102,  0.3393, -0.4293],\n",
      "         [-0.7932, -0.0890, -0.2130,  ..., -0.7117,  0.2915, -0.2182],\n",
      "         [-0.8613, -0.2703, -0.3654,  ..., -0.7263,  0.1430, -0.3292],\n",
      "         ...,\n",
      "         [-0.6305, -0.4551, -0.5146,  ..., -0.7780,  0.2512, -0.6030],\n",
      "         [-0.5449, -0.2584, -0.3540,  ..., -0.6833,  0.3796, -0.3651],\n",
      "         [-0.5614, -0.1897, -0.3873,  ..., -0.6882,  0.2852, -0.2929]]],\n",
      "       device='cuda:0'), tensor([[[-0.7994, -0.7280, -0.4998,  ...,  0.0102,  0.3393, -0.4293],\n",
      "         [-0.7932, -0.0890, -0.2130,  ..., -0.7117,  0.2915, -0.2182],\n",
      "         [-0.8613, -0.2703, -0.3654,  ..., -0.7263,  0.1430, -0.3292],\n",
      "         ...,\n",
      "         [-0.6305, -0.4551, -0.5146,  ..., -0.7780,  0.2512, -0.6030],\n",
      "         [-0.5449, -0.2584, -0.3540,  ..., -0.6833,  0.3796, -0.3651],\n",
      "         [-0.5614, -0.1897, -0.3873,  ..., -0.6882,  0.2852, -0.2929]]],\n",
      "       device='cuda:0'), tensor([[[-0.7994, -0.7280, -0.4998,  ...,  0.0102,  0.3393, -0.4293],\n",
      "         [-0.7932, -0.0890, -0.2130,  ..., -0.7117,  0.2915, -0.2182],\n",
      "         [-0.8613, -0.2703, -0.3654,  ..., -0.7263,  0.1430, -0.3292],\n",
      "         ...,\n",
      "         [-0.6305, -0.4551, -0.5146,  ..., -0.7780,  0.2512, -0.6030],\n",
      "         [-0.5449, -0.2584, -0.3540,  ..., -0.6833,  0.3796, -0.3651],\n",
      "         [-0.5614, -0.1897, -0.3873,  ..., -0.6882,  0.2852, -0.2929]]],\n",
      "       device='cuda:0')] \n",
      " *** \n",
      "\n",
      "\n",
      " *** \n",
      " image_features: tensor([[ 7.4475e-04,  9.4201e-02,  3.2347e-02,  5.9139e-03, -4.6069e-02,\n",
      "         -5.9820e-03,  3.8616e-03, -2.8322e-03,  2.3203e-02,  5.4476e-03,\n",
      "          2.4138e-02, -4.6762e-02, -8.0957e-03,  6.5805e-02,  1.2504e-02,\n",
      "         -1.5383e-02, -5.1786e-02, -2.9469e-02,  4.2362e-02, -3.4583e-02,\n",
      "          4.9548e-03,  1.2656e-02,  1.5244e-02,  2.6495e-02, -7.1202e-02,\n",
      "         -1.3832e-02, -3.2830e-02, -5.1882e-02,  2.3932e-02, -4.2924e-03,\n",
      "          2.7858e-02, -2.1646e-02, -3.9252e-02, -4.7776e-03,  2.1804e-02,\n",
      "          4.5928e-02, -8.2110e-03, -1.2172e-02,  2.6399e-02, -2.0002e-03,\n",
      "          1.0502e-02, -8.9821e-03,  4.0830e-02, -5.5815e-02, -3.3665e-02,\n",
      "          2.0811e-02, -3.2716e-03,  4.0783e-02,  1.0831e-02, -4.5190e-03,\n",
      "          1.9942e-02,  8.1530e-03,  2.1371e-02,  1.0205e-02, -4.4277e-02,\n",
      "         -8.7759e-03,  1.8627e-02, -6.5175e-02,  5.3937e-02, -1.7503e-02,\n",
      "         -7.1298e-03, -9.7934e-03, -7.3682e-03,  6.7639e-03,  5.9814e-02,\n",
      "          3.6948e-02,  1.7890e-02,  2.7138e-02, -6.9796e-03, -4.8740e-02,\n",
      "          3.0624e-02,  8.8913e-03,  3.6387e-02, -1.2898e-02,  1.0188e-02,\n",
      "          2.9303e-02, -5.6770e-02, -1.3008e-02, -6.5942e-04,  2.9148e-02,\n",
      "          8.2150e-03,  1.5716e-02,  9.7246e-03,  1.9439e-02,  3.3009e-02,\n",
      "         -1.2652e-02,  5.7524e-02,  9.4944e-03,  7.5356e-02, -2.7807e-02,\n",
      "          3.7223e-02, -9.3530e-03,  3.7419e-02, -1.6789e-02, -9.2171e-03,\n",
      "         -3.0882e-02,  1.6855e-02,  3.2550e-04,  1.1101e-02, -5.3582e-02,\n",
      "         -2.7924e-02,  2.6170e-02, -1.8383e-02, -2.2850e-03, -2.3664e-02,\n",
      "         -2.7394e-03, -4.0353e-02, -1.0624e-02, -6.3793e-02, -4.8051e-02,\n",
      "         -8.5555e-03,  1.2195e-03,  1.9144e-02, -4.0994e-02, -9.1897e-03,\n",
      "         -6.0485e-03, -2.0261e-02, -3.3437e-02, -3.9164e-02,  1.4061e-02,\n",
      "         -2.4402e-02, -6.3308e-03,  3.6178e-02,  7.7575e-03, -8.6015e-03,\n",
      "         -1.9383e-03,  1.3572e-02, -1.8947e-02, -3.4034e-02,  1.5425e-02,\n",
      "         -3.2260e-02,  3.8422e-02,  8.9092e-03, -1.1249e-02, -6.3902e-03,\n",
      "          3.4861e-02, -4.2561e-02, -1.1225e-02,  5.3666e-02,  2.2788e-03,\n",
      "         -3.0553e-03,  1.3373e-02, -4.1625e-02,  1.6959e-03,  4.9275e-02,\n",
      "          3.2440e-02, -4.7710e-02, -2.2997e-02, -3.4914e-02,  2.9040e-02,\n",
      "         -4.9904e-02,  4.3795e-02,  3.9057e-02,  4.6553e-03, -3.5920e-02,\n",
      "          3.4657e-02,  2.2598e-02,  1.9006e-02,  1.1925e-02, -2.9999e-02,\n",
      "          6.1269e-03,  3.2561e-03,  1.6428e-02, -3.4471e-02, -2.7138e-03,\n",
      "         -9.2802e-03, -1.9603e-02,  1.3283e-02, -4.3394e-02,  2.1231e-02,\n",
      "          2.4934e-02, -1.0605e-02,  3.1218e-02,  4.9791e-02, -4.5541e-02,\n",
      "         -1.5273e-02, -1.9081e-02,  9.0869e-02,  8.2183e-02,  2.7062e-02,\n",
      "          8.1813e-02,  5.6757e-04, -8.3085e-03, -2.8739e-02,  7.0068e-03,\n",
      "         -1.4485e-03, -7.6359e-02,  2.6863e-02, -8.9124e-03, -1.7459e-02,\n",
      "         -3.2773e-03,  2.6128e-02,  7.4940e-03, -1.7415e-02, -3.6725e-02,\n",
      "          4.4385e-02, -4.1290e-02,  5.8918e-02,  4.2263e-02, -9.8246e-03,\n",
      "         -3.5525e-03, -1.3740e-01,  1.4542e-02,  1.4252e-03, -4.5275e-02,\n",
      "          5.8602e-02,  1.4527e-02, -7.8899e-03,  2.3625e-02,  2.1697e-02,\n",
      "          2.6380e-02, -2.3803e-02, -9.3079e-03,  3.2653e-02,  1.9627e-02,\n",
      "          3.5798e-02,  5.6574e-03, -3.9869e-02, -2.4390e-03, -4.2174e-02,\n",
      "         -4.2999e-02, -4.0495e-02,  1.9181e-02,  2.4773e-02, -2.6202e-02,\n",
      "         -9.7252e-03, -1.3767e-02, -2.7732e-02,  4.7310e-02, -1.0559e-02,\n",
      "         -6.8557e-02, -8.1876e-03,  2.6786e-02, -7.7874e-02,  3.1926e-02,\n",
      "          2.8634e-02, -8.5475e-03, -4.0101e-02,  4.4407e-02,  2.1814e-02,\n",
      "         -5.6565e-02,  3.5543e-02,  1.2142e-02, -2.8901e-02, -6.3378e-02,\n",
      "          6.0336e-03, -2.3609e-02,  1.6680e-02,  2.1156e-04,  3.1647e-02,\n",
      "          3.7126e-02, -2.0603e-03,  4.3212e-02,  7.7804e-03, -2.5983e-02,\n",
      "         -8.5015e-03,  2.8565e-02,  6.3147e-02, -4.1421e-02,  7.5255e-02,\n",
      "         -3.5930e-02, -4.7561e-02,  1.5330e-02, -3.1561e-03, -3.4165e-02,\n",
      "          1.9800e-02,  1.8975e-02,  8.2823e-02, -6.7324e-02, -1.5525e-02,\n",
      "          3.8316e-03,  5.5192e-02, -6.8651e-03,  3.4666e-02,  1.0422e-02,\n",
      "         -6.8198e-03,  4.2108e-04,  7.9494e-03,  2.8201e-02,  2.3574e-02,\n",
      "         -2.1997e-02,  2.3933e-02,  3.2366e-02, -2.4441e-03, -1.4682e-02,\n",
      "          2.4933e-02,  2.0975e-02, -9.1037e-03,  4.3796e-02, -1.0021e-02,\n",
      "         -5.8320e-03, -1.6596e-02, -1.6719e-02,  4.1348e-02,  6.0433e-02,\n",
      "         -9.3210e-03, -1.3383e-02, -9.5141e-03, -5.3271e-02, -1.3415e-03,\n",
      "          5.1287e-02,  5.6864e-03, -1.6740e-03,  2.5585e-02,  8.9791e-03,\n",
      "          5.2636e-02, -2.8247e-02,  3.0525e-02, -5.1152e-02,  3.9542e-02,\n",
      "          2.4609e-02,  7.3347e-03,  2.1187e-02,  4.5616e-02,  2.5430e-02,\n",
      "          5.5158e-03, -2.7255e-02,  2.1025e-02, -3.3618e-03,  3.3223e-02,\n",
      "         -3.8345e-02, -2.7003e-02, -2.3568e-02,  5.4668e-02, -6.2701e-04,\n",
      "          4.3621e-03, -2.3657e-02, -7.4869e-02, -4.3627e-02,  6.3057e-03,\n",
      "          5.7977e-02, -1.5234e-02,  1.4988e-03, -4.1235e-02, -4.5989e-02,\n",
      "         -4.8993e-03, -4.7115e-02,  1.4762e-03, -3.8738e-02,  1.6818e-02,\n",
      "         -2.1619e-02, -1.9806e-02,  2.7621e-02, -3.3682e-02, -1.5349e-02,\n",
      "         -1.6487e-02,  1.3734e-02, -7.8882e-03,  4.9983e-03, -4.1951e-02,\n",
      "          2.7351e-02,  7.3187e-03, -7.4890e-02,  1.3535e-02, -1.8446e-02,\n",
      "         -2.7473e-02,  9.3856e-03, -1.3784e-02,  3.5622e-02, -1.3936e-02,\n",
      "          4.0166e-02,  3.2860e-02,  1.9826e-02,  7.6313e-02, -3.2766e-02,\n",
      "         -1.5735e-02,  1.9983e-02, -3.9101e-02,  4.7175e-02, -2.6561e-02,\n",
      "         -3.6118e-02,  1.3287e-02, -6.9289e-02, -4.0440e-03,  4.6442e-02,\n",
      "          4.7638e-02,  9.9303e-03,  2.4744e-02,  4.7862e-02, -3.7843e-02,\n",
      "         -1.6030e-02, -2.0092e-02, -1.0433e-02, -2.4414e-02, -2.1011e-02,\n",
      "         -1.4777e-02,  3.0381e-02,  1.5938e-03,  1.0931e-03,  9.8638e-03,\n",
      "         -3.7594e-02, -1.8784e-03,  1.7311e-02, -2.4108e-02,  1.6849e-02,\n",
      "          8.2214e-03,  1.8415e-02,  1.9048e-02, -2.4511e-03,  3.1427e-02,\n",
      "          9.3915e-02,  1.6163e-02, -1.4478e-02, -2.7093e-02, -1.4291e-01,\n",
      "          8.9191e-03, -5.7459e-02,  1.2144e-02,  5.3856e-02, -1.6355e-02,\n",
      "          1.7444e-03, -8.5261e-03,  4.2135e-02, -2.4705e-01, -3.1566e-02,\n",
      "         -2.7289e-02,  3.6698e-02, -4.5017e-02,  8.7242e-02,  9.3781e-03,\n",
      "          1.1694e-03, -6.1865e-03,  1.1351e-02, -2.3166e-02,  1.1747e-02,\n",
      "         -1.6699e-02,  1.4961e-02,  1.4010e-02,  6.3132e-02,  2.8575e-02,\n",
      "          9.9378e-03,  4.1232e-02, -2.3244e-02, -2.3183e-03, -6.8024e-02,\n",
      "         -7.9842e-03,  1.8347e-02,  2.2003e-02,  5.8394e-03, -1.6143e-02,\n",
      "          1.1776e-02, -1.7284e-03,  1.4855e-02, -1.1092e-02,  2.6631e-03,\n",
      "          3.9691e-02,  1.7611e-03, -9.1898e-03,  3.7980e-03, -2.0245e-02,\n",
      "         -4.0619e-02, -1.3073e-02, -6.5085e-02,  6.5025e-02,  4.1075e-02,\n",
      "         -2.9987e-02,  1.3131e-03,  4.8474e-02,  8.3615e-03,  3.8417e-02,\n",
      "         -1.6306e-02,  2.1701e-02, -1.5596e-02, -1.5969e-04, -4.5353e-02,\n",
      "          1.9371e-02, -1.8932e-03, -8.0760e-03, -2.7890e-02, -5.8758e-02,\n",
      "          2.5649e-02,  2.6361e-02,  3.4244e-02,  9.1788e-03,  1.8674e-02,\n",
      "         -5.6257e-02,  2.9455e-02,  2.1814e-02, -8.6556e-03, -2.0547e-02,\n",
      "         -1.8518e-02,  3.5604e-02,  1.2125e-02, -2.4062e-02, -2.1574e-02,\n",
      "          7.8239e-03, -8.3122e-03, -1.7355e-02,  1.1319e-02,  1.4227e-02,\n",
      "          3.0960e-02, -2.5456e-02, -1.2884e-02,  8.4085e-02, -2.3128e-02,\n",
      "          1.2117e-03, -1.2336e-02, -3.6334e-03,  3.1631e-02,  5.2630e-02,\n",
      "          1.9291e-02,  2.9679e-03, -2.1524e-02,  3.2391e-02, -7.0811e-02,\n",
      "          1.0505e-02, -4.1947e-03, -4.3445e-03, -1.4685e-02,  2.0211e-02,\n",
      "         -3.2426e-02, -1.7512e-03,  4.7026e-02, -1.7215e-02, -1.4398e-02,\n",
      "         -1.8506e-02, -2.8237e-03,  1.3990e-02, -5.2307e-02,  2.7976e-02,\n",
      "         -6.3363e-03, -2.1760e-02,  5.0196e-02, -6.4104e-02,  2.4861e-02,\n",
      "          1.8821e-03,  5.2468e-02,  1.8820e-02, -8.8773e-02,  8.5479e-03,\n",
      "          1.9375e-02, -4.5483e-02, -2.1367e-02, -2.8056e-02,  3.5923e-02,\n",
      "         -1.8221e-02,  4.3539e-02, -1.1304e-02, -3.0300e-02,  6.1455e-02,\n",
      "          8.4442e-03,  5.1073e-02,  5.0669e-02, -5.8037e-03, -8.8235e-03,\n",
      "         -1.2699e-02,  4.5018e-02,  2.5290e-02, -3.3060e-03,  2.7198e-02,\n",
      "          1.1135e-02,  1.8133e-02,  4.5866e-02, -1.8697e-02,  1.0251e-02,\n",
      "         -4.9311e-03,  7.2229e-02,  9.8147e-03, -1.0306e-02, -8.9407e-03,\n",
      "          2.1357e-02, -6.9670e-04,  1.1210e-02,  4.0433e-02,  3.9949e-02,\n",
      "          3.2125e-02, -2.1368e-02,  2.2349e-02,  3.5051e-02,  7.8843e-04,\n",
      "          2.8654e-02, -2.7809e-02, -1.3282e-02, -1.0666e-02,  4.6000e-02,\n",
      "         -3.5298e-02, -1.8482e-02, -3.0216e-02,  1.5603e-03, -6.9442e-02,\n",
      "          3.2317e-02, -2.1685e-02, -3.4773e-02,  1.5351e-02, -3.9283e-02,\n",
      "          3.6526e-02, -2.0477e-02, -1.5677e-03,  1.6423e-03, -1.1827e-02,\n",
      "         -3.1498e-02,  5.0394e-03, -3.8357e-02, -6.2265e-02, -1.3039e-02,\n",
      "         -1.5931e-02,  5.3070e-02, -2.3879e-02, -5.3774e-02,  1.8292e-02,\n",
      "         -5.9505e-02,  1.3988e-03,  1.5610e-02, -1.4866e-02,  3.8467e-02,\n",
      "         -8.4360e-02, -1.5830e-02, -1.5245e-02, -1.9687e-02, -2.8468e-02,\n",
      "          2.1943e-02, -5.6758e-02, -9.4223e-02,  7.3114e-02,  1.7002e-02,\n",
      "          1.0372e-02,  5.7860e-02, -1.4312e-02,  4.2960e-02,  2.5507e-03,\n",
      "         -3.9734e-02, -3.8598e-03, -1.5112e-02, -3.5374e-02, -3.3164e-02,\n",
      "         -2.3730e-02,  3.1851e-02, -6.5289e-02,  5.3772e-02,  2.2400e-02,\n",
      "         -3.0592e-03, -1.0759e-02, -2.9909e-02,  3.6031e-02, -4.9795e-02,\n",
      "          6.7244e-02,  3.7378e-02, -1.0918e-01,  3.4050e-02, -5.6337e-02,\n",
      "          1.5095e-02,  4.9910e-02,  3.1432e-02, -8.3713e-03,  2.6559e-01,\n",
      "          2.4215e-02,  1.8956e-02,  5.6933e-02,  2.7495e-03, -2.4451e-03,\n",
      "         -5.2396e-02, -2.4567e-02, -3.0662e-02, -1.4272e-03, -2.6346e-02,\n",
      "          5.9496e-03, -5.8532e-03, -7.4540e-03,  4.4828e-02,  3.9583e-02,\n",
      "         -2.2281e-02, -5.1477e-02,  7.0419e-02,  3.0825e-02,  3.3580e-02,\n",
      "          1.4136e-02,  1.6409e-02, -3.5497e-02,  3.9803e-02,  2.3071e-02,\n",
      "          6.2582e-03, -9.9065e-03, -6.2112e-02,  5.9818e-03, -1.0736e-02,\n",
      "          3.7776e-02,  3.6253e-02,  3.9072e-02, -3.4082e-02, -6.8298e-02,\n",
      "         -1.4041e-02,  2.7507e-02, -2.7869e-02, -7.8567e-03, -3.7279e-02,\n",
      "         -1.4157e-03,  4.2087e-02, -2.2428e-02, -3.8272e-02, -6.0232e-03,\n",
      "         -4.4701e-03, -4.1323e-03,  1.9025e-01,  5.9688e-03, -1.3562e-02,\n",
      "          2.2973e-02, -3.5114e-02, -6.0446e-02, -6.7451e-04,  1.2685e-02,\n",
      "          7.0825e-03, -2.0445e-02, -7.7305e-03, -3.0245e-02,  5.0389e-02,\n",
      "         -3.5456e-02,  9.6345e-03, -1.7553e-02, -1.3164e-02, -1.6705e-02,\n",
      "          2.9442e-03,  3.2621e-02,  2.2482e-02, -1.5775e-03, -5.2921e-02,\n",
      "         -4.6901e-02,  4.1173e-02, -1.1140e-02, -1.3941e-02,  9.9072e-03,\n",
      "          1.4781e-02, -1.9733e-02, -2.6454e-02, -3.6376e-02, -5.3406e-03,\n",
      "         -2.8944e-02,  5.8442e-03,  4.5736e-03,  1.6384e-02,  1.2175e-02,\n",
      "         -4.1931e-02, -4.1799e-03,  4.1509e-03, -1.1176e-02,  4.6910e-02,\n",
      "          1.2213e-03,  3.1765e-02,  3.2480e-04,  9.2552e-04,  1.4862e-02,\n",
      "          1.4200e-02, -3.3395e-02,  2.9016e-02,  2.5837e-02, -6.0903e-03,\n",
      "         -5.1714e-03,  5.6076e-02, -2.2590e-02, -2.6156e-02, -1.1635e-02,\n",
      "          1.0034e-02,  4.3855e-03,  3.2570e-02, -3.2838e-02, -3.8770e-02,\n",
      "         -2.1113e-02, -1.0845e-02, -5.5191e-02, -3.9510e-02,  1.6405e-03,\n",
      "          1.0149e-02, -1.5907e-02,  4.9509e-02,  1.0001e-01, -8.9261e-03,\n",
      "          1.5120e-02, -1.1645e-02,  7.4470e-03]], device='cuda:0') \n",
      " *** \n",
      "\n",
      "\n",
      " *** \n",
      " text_probs 1 : tensor([[[-0.0634,  0.1712]]], device='cuda:0') \n",
      " *** \n",
      "\n",
      "\n",
      " *** \n",
      " text_probs 2 : tensor([[[0.0339, 0.9661]]], device='cuda:0') \n",
      " *** \n",
      "\n",
      "\n",
      " *** \n",
      " text_probs 3 : tensor([0.9661], device='cuda:0') \n",
      " *** \n",
      "\n",
      "\n",
      " *** \n",
      " anomaly_map: tensor([[[[0.0143, 0.0143, 0.0143,  ..., 0.0059, 0.0059, 0.0059],\n",
      "          [0.0143, 0.0143, 0.0143,  ..., 0.0059, 0.0059, 0.0059],\n",
      "          [0.0143, 0.0143, 0.0143,  ..., 0.0059, 0.0059, 0.0059],\n",
      "          ...,\n",
      "          [0.0181, 0.0181, 0.0181,  ..., 0.0114, 0.0114, 0.0114],\n",
      "          [0.0181, 0.0181, 0.0181,  ..., 0.0114, 0.0114, 0.0114],\n",
      "          [0.0181, 0.0181, 0.0181,  ..., 0.0114, 0.0114, 0.0114]]],\n",
      "\n",
      "\n",
      "        [[[0.0143, 0.0143, 0.0143,  ..., 0.0059, 0.0059, 0.0059],\n",
      "          [0.0143, 0.0143, 0.0143,  ..., 0.0059, 0.0059, 0.0059],\n",
      "          [0.0143, 0.0143, 0.0143,  ..., 0.0059, 0.0059, 0.0059],\n",
      "          ...,\n",
      "          [0.0181, 0.0181, 0.0181,  ..., 0.0114, 0.0114, 0.0114],\n",
      "          [0.0181, 0.0181, 0.0181,  ..., 0.0114, 0.0114, 0.0114],\n",
      "          [0.0181, 0.0181, 0.0181,  ..., 0.0114, 0.0114, 0.0114]]],\n",
      "\n",
      "\n",
      "        [[[0.0143, 0.0143, 0.0143,  ..., 0.0059, 0.0059, 0.0059],\n",
      "          [0.0143, 0.0143, 0.0143,  ..., 0.0059, 0.0059, 0.0059],\n",
      "          [0.0143, 0.0143, 0.0143,  ..., 0.0059, 0.0059, 0.0059],\n",
      "          ...,\n",
      "          [0.0181, 0.0181, 0.0181,  ..., 0.0114, 0.0114, 0.0114],\n",
      "          [0.0181, 0.0181, 0.0181,  ..., 0.0114, 0.0114, 0.0114],\n",
      "          [0.0181, 0.0181, 0.0181,  ..., 0.0114, 0.0114, 0.0114]]],\n",
      "\n",
      "\n",
      "        [[[0.0143, 0.0143, 0.0143,  ..., 0.0059, 0.0059, 0.0059],\n",
      "          [0.0143, 0.0143, 0.0143,  ..., 0.0059, 0.0059, 0.0059],\n",
      "          [0.0143, 0.0143, 0.0143,  ..., 0.0059, 0.0059, 0.0059],\n",
      "          ...,\n",
      "          [0.0181, 0.0181, 0.0181,  ..., 0.0114, 0.0114, 0.0114],\n",
      "          [0.0181, 0.0181, 0.0181,  ..., 0.0114, 0.0114, 0.0114],\n",
      "          [0.0181, 0.0181, 0.0181,  ..., 0.0114, 0.0114, 0.0114]]]],\n",
      "       device='cuda:0') \n",
      " *** \n",
      "\n",
      "\n",
      " *** \n",
      " anomaly_map_sum: tensor([[[0.0572, 0.0572, 0.0572,  ..., 0.0235, 0.0235, 0.0235],\n",
      "         [0.0572, 0.0572, 0.0572,  ..., 0.0235, 0.0235, 0.0235],\n",
      "         [0.0572, 0.0572, 0.0572,  ..., 0.0235, 0.0235, 0.0235],\n",
      "         ...,\n",
      "         [0.0724, 0.0724, 0.0724,  ..., 0.0455, 0.0455, 0.0455],\n",
      "         [0.0724, 0.0724, 0.0724,  ..., 0.0455, 0.0455, 0.0455],\n",
      "         [0.0724, 0.0724, 0.0724,  ..., 0.0455, 0.0455, 0.0455]]],\n",
      "       device='cuda:0') \n",
      " *** \n",
      "\n",
      "\n",
      " *** \n",
      " results: {'orange': {'gt_sp': [tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1)], 'pr_sp': [tensor(0.9661), tensor(0.9661), tensor(0.9661), tensor(0.9661), tensor(0.9661), tensor(0.9661), tensor(0.9661)], 'imgs_masks': [tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]]), tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]]), tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]]), tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]]), tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]]), tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]]), tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]])], 'anomaly_maps': [tensor([[[0.0564, 0.0563, 0.0562,  ..., 0.0235, 0.0234, 0.0234],\n",
      "         [0.0563, 0.0562, 0.0560,  ..., 0.0234, 0.0234, 0.0234],\n",
      "         [0.0560, 0.0559, 0.0557,  ..., 0.0234, 0.0234, 0.0234],\n",
      "         ...,\n",
      "         [0.0721, 0.0721, 0.0720,  ..., 0.0451, 0.0452, 0.0452],\n",
      "         [0.0722, 0.0722, 0.0721,  ..., 0.0452, 0.0452, 0.0453],\n",
      "         [0.0722, 0.0722, 0.0722,  ..., 0.0452, 0.0453, 0.0453]]]), tensor([[[0.0564, 0.0563, 0.0562,  ..., 0.0235, 0.0234, 0.0234],\n",
      "         [0.0563, 0.0562, 0.0560,  ..., 0.0234, 0.0234, 0.0234],\n",
      "         [0.0560, 0.0559, 0.0557,  ..., 0.0234, 0.0234, 0.0234],\n",
      "         ...,\n",
      "         [0.0721, 0.0721, 0.0720,  ..., 0.0451, 0.0452, 0.0452],\n",
      "         [0.0722, 0.0722, 0.0721,  ..., 0.0452, 0.0452, 0.0453],\n",
      "         [0.0722, 0.0722, 0.0722,  ..., 0.0452, 0.0453, 0.0453]]]), tensor([[[0.0564, 0.0563, 0.0562,  ..., 0.0235, 0.0234, 0.0234],\n",
      "         [0.0563, 0.0562, 0.0560,  ..., 0.0234, 0.0234, 0.0234],\n",
      "         [0.0560, 0.0559, 0.0557,  ..., 0.0234, 0.0234, 0.0234],\n",
      "         ...,\n",
      "         [0.0721, 0.0721, 0.0720,  ..., 0.0451, 0.0452, 0.0452],\n",
      "         [0.0722, 0.0722, 0.0721,  ..., 0.0452, 0.0452, 0.0453],\n",
      "         [0.0722, 0.0722, 0.0722,  ..., 0.0452, 0.0453, 0.0453]]]), tensor([[[0.0564, 0.0563, 0.0562,  ..., 0.0235, 0.0234, 0.0234],\n",
      "         [0.0563, 0.0562, 0.0560,  ..., 0.0234, 0.0234, 0.0234],\n",
      "         [0.0560, 0.0559, 0.0557,  ..., 0.0234, 0.0234, 0.0234],\n",
      "         ...,\n",
      "         [0.0721, 0.0721, 0.0720,  ..., 0.0451, 0.0452, 0.0452],\n",
      "         [0.0722, 0.0722, 0.0721,  ..., 0.0452, 0.0452, 0.0453],\n",
      "         [0.0722, 0.0722, 0.0722,  ..., 0.0452, 0.0453, 0.0453]]]), tensor([[[0.0564, 0.0563, 0.0562,  ..., 0.0235, 0.0234, 0.0234],\n",
      "         [0.0563, 0.0562, 0.0560,  ..., 0.0234, 0.0234, 0.0234],\n",
      "         [0.0560, 0.0559, 0.0557,  ..., 0.0234, 0.0234, 0.0234],\n",
      "         ...,\n",
      "         [0.0721, 0.0721, 0.0720,  ..., 0.0451, 0.0452, 0.0452],\n",
      "         [0.0722, 0.0722, 0.0721,  ..., 0.0452, 0.0452, 0.0453],\n",
      "         [0.0722, 0.0722, 0.0722,  ..., 0.0452, 0.0453, 0.0453]]]), tensor([[[0.0564, 0.0563, 0.0562,  ..., 0.0235, 0.0234, 0.0234],\n",
      "         [0.0563, 0.0562, 0.0560,  ..., 0.0234, 0.0234, 0.0234],\n",
      "         [0.0560, 0.0559, 0.0557,  ..., 0.0234, 0.0234, 0.0234],\n",
      "         ...,\n",
      "         [0.0721, 0.0721, 0.0720,  ..., 0.0451, 0.0452, 0.0452],\n",
      "         [0.0722, 0.0722, 0.0721,  ..., 0.0452, 0.0452, 0.0453],\n",
      "         [0.0722, 0.0722, 0.0722,  ..., 0.0452, 0.0453, 0.0453]]]), tensor([[[0.0564, 0.0563, 0.0562,  ..., 0.0235, 0.0234, 0.0234],\n",
      "         [0.0563, 0.0562, 0.0560,  ..., 0.0234, 0.0234, 0.0234],\n",
      "         [0.0560, 0.0559, 0.0557,  ..., 0.0234, 0.0234, 0.0234],\n",
      "         ...,\n",
      "         [0.0721, 0.0721, 0.0720,  ..., 0.0451, 0.0452, 0.0452],\n",
      "         [0.0722, 0.0722, 0.0721,  ..., 0.0452, 0.0452, 0.0453],\n",
      "         [0.0722, 0.0722, 0.0722,  ..., 0.0452, 0.0453, 0.0453]]])]}} \n",
      " *** \n",
      "\n",
      "\n",
      "\n",
      "***** [tensor(0.9661), tensor(0.9661), tensor(0.9661), tensor(0.9661), tensor(0.9661), tensor(0.9661), tensor(0.9661)] *****\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "join() argument must be str, bytes, or os.PathLike object, not 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[47], line 42\u001b[0m\n\u001b[0;32m     40\u001b[0m     results[cls_name[\u001b[38;5;241m0\u001b[39m]][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124manomaly_maps\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(anomaly_map)\n\u001b[0;32m     41\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m *** \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m results: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresults\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m *** \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 42\u001b[0m     \u001b[43mvisualizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mitems\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mimg_path\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43manomaly_map\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetach\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimage_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcls_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresults\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcls_name\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpr_sp\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m idx \u001b[38;5;241m!=\u001b[39m\u001b[38;5;241m0\u001b[39m :\n\u001b[0;32m     44\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\dmkwo\\Documents\\soo\\AnomalyCLIP\\visualization.py:15\u001b[0m, in \u001b[0;36mvisualizer\u001b[1;34m(pathes, anomaly_map, img_size, save_path, cls_name, pr_sp)\u001b[0m\n\u001b[0;32m     13\u001b[0m vis \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mcvtColor(vis, cv2\u001b[38;5;241m.\u001b[39mCOLOR_RGB2BGR)  \u001b[38;5;66;03m# BGR\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pr_sp \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0.5\u001b[39m:\n\u001b[1;32m---> 15\u001b[0m     save_vis \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(save_path, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mabn_imgs\u001b[39m\u001b[38;5;124m'\u001b[39m, cls_name[idx], \u001b[38;5;28mcls\u001b[39m)\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(save_vis):\n\u001b[0;32m     17\u001b[0m         os\u001b[38;5;241m.\u001b[39mmakedirs(save_vis)\n",
      "File \u001b[1;32mc:\\Users\\dmkwo\\anaconda3\\envs\\drct\\lib\\ntpath.py:117\u001b[0m, in \u001b[0;36mjoin\u001b[1;34m(path, *paths)\u001b[0m\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result_drive \u001b[38;5;241m+\u001b[39m result_path\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mTypeError\u001b[39;00m, \u001b[38;5;167;01mAttributeError\u001b[39;00m, \u001b[38;5;167;01mBytesWarning\u001b[39;00m):\n\u001b[1;32m--> 117\u001b[0m     \u001b[43mgenericpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_arg_types\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mjoin\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpaths\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    118\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\dmkwo\\anaconda3\\envs\\drct\\lib\\genericpath.py:152\u001b[0m, in \u001b[0;36m_check_arg_types\u001b[1;34m(funcname, *args)\u001b[0m\n\u001b[0;32m    150\u001b[0m         hasbytes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    151\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 152\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfuncname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m() argument must be str, bytes, or \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    153\u001b[0m                         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mos.PathLike object, not \u001b[39m\u001b[38;5;132;01m{\u001b[39;00ms\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    154\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m hasstr \u001b[38;5;129;01mand\u001b[39;00m hasbytes:\n\u001b[0;32m    155\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCan\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt mix strings and bytes in path components\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: join() argument must be str, bytes, or os.PathLike object, not 'list'"
     ]
    }
   ],
   "source": [
    "from visualization import visualizer\n",
    "\n",
    "\n",
    "for idx, items in enumerate(tqdm(test_dataloader)):\n",
    "    image = items['img'].to(device)\n",
    "    cls_name = items['cls_name']\n",
    "    cls_id = items['cls_id']\n",
    "    gt_mask = items['img_mask']\n",
    "    gt_mask[gt_mask > 0.5], gt_mask[gt_mask <= 0.5] = 1, 0\n",
    "    results[cls_name[0]]['imgs_masks'].append(gt_mask)  # px\n",
    "    results[cls_name[0]]['gt_sp'].extend(items['anomaly'].detach().cpu())\n",
    "\n",
    "    with torch.no_grad():\n",
    "        image_features, patch_features = model.encode_image(image, features_list, DPAM_layer = 20)\n",
    "        print(f'\\n *** \\n image_features: {image_features} \\n patch_features:{patch_features} \\n *** \\n')\n",
    "        image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n",
    "        print(f'\\n *** \\n image_features: {image_features} \\n *** \\n')\n",
    "        \n",
    "        text_probs = image_features @ text_features.permute(0, 2, 1)\n",
    "        print(f'\\n *** \\n text_probs 1 : {text_probs} \\n *** \\n')\n",
    "        text_probs = (text_probs/0.07).softmax(-1)\n",
    "        print(f'\\n *** \\n text_probs 2 : {text_probs} \\n *** \\n')\n",
    "        text_probs = text_probs[:, 0, 1] # anomaly score = P(g_a, x)\n",
    "        print(f'\\n *** \\n text_probs 3 : {text_probs} \\n *** \\n')\n",
    "        anomaly_map_list = []\n",
    "        for idx, patch_feature in enumerate(patch_features): #  anomlay score\n",
    "            if idx >= args.feature_map_layer[0]:\n",
    "                patch_feature = patch_feature/ patch_feature.norm(dim = -1, keepdim = True)\n",
    "                similarity, _ = AnomalyCLIP_lib.compute_similarity(patch_feature, text_features[0])\n",
    "                similarity_map = AnomalyCLIP_lib.get_similarity_map(similarity[:, 1:, :], args.image_size)\n",
    "                anomaly_map = (similarity_map[...,1] + 1 - similarity_map[...,0])/2.0\n",
    "                anomaly_map_list.append(anomaly_map)\n",
    "\n",
    "        anomaly_map = torch.stack(anomaly_map_list)\n",
    "        print(f'\\n *** \\n anomaly_map: {anomaly_map} \\n *** \\n')\n",
    "        anomaly_map = anomaly_map.sum(dim = 0)\n",
    "        print(f'\\n *** \\n anomaly_map_sum: {anomaly_map} \\n *** \\n')\n",
    "        results[cls_name[0]]['pr_sp'].extend(text_probs.detach().cpu())\n",
    "        anomaly_map = torch.stack([torch.from_numpy(gaussian_filter(i, sigma = args.sigma)) for i in anomaly_map.detach().cpu()], dim = 0 )\n",
    "        results[cls_name[0]]['anomaly_maps'].append(anomaly_map)\n",
    "        print(f'\\n *** \\n results: {results} \\n *** \\n')\n",
    "        visualizer(items['img_path'], anomaly_map.detach().cpu().numpy(), args.image_size, args.save_path, cls_name, results[cls_name[0]]['pr_sp'])\n",
    "    if idx !=0 :\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['gt_sp', 'pr_sp', 'imgs_masks', 'anomaly_maps'])\n"
     ]
    }
   ],
   "source": [
    "print(results['orange'].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor(1)] [tensor(0.9661)]\n"
     ]
    }
   ],
   "source": [
    "print(results['orange']['gt_sp'], results['orange']['pr_sp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anomalyCLIP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
